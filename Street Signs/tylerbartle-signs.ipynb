{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "StiU5QcPPxqQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-10 14:24:36.104555: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-10 14:24:36.249691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741638276.302724   65669 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741638276.317657   65669 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-10 14:24:36.450048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import cv2\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "from tensorflow.data import Dataset # type: ignore\n",
        "from tensorflow.keras import layers, mixed_precision, Sequential # type: ignore\n",
        "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB3 # type: ignore\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D, SpatialDropout2D # type: ignore\n",
        "from tensorflow.keras.models import Model # type: ignore\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\"\n",
        "\n",
        "tf.config.set_logical_device_configuration(\n",
        "    gpus[0],\n",
        "    [tf.config.LogicalDeviceConfiguration(memory_limit=14000)]  # Adjust memory limit (MB) if needed\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(tf.sysconfig.get_build_info())\n",
        "# print(f'tensorflow version {tf.__version__}')\n",
        "# print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "# print(tf.config.list_physical_devices('GPU'))\n",
        "# print(tf.config.list_logical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA0HPVmIBT4C"
      },
      "outputs": [],
      "source": [
        "# !echo \"Downloading files...\"\n",
        "# !wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training1.zip\n",
        "# !wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/training2.zip\n",
        "# !wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/holdout.zip\n",
        "# !wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/mini_holdout.zip\n",
        "# !wget -q https://github.com/byui-cse/cse450-course/raw/master/data/roadsigns/mini_holdout_answers.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upscaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resized 52040 images and saved them to upscaled\n"
          ]
        }
      ],
      "source": [
        "### This has been done already, no need to do again.\n",
        "# # Define paths\n",
        "# input_folder = \"original\"\n",
        "# output_folder = \"upscaled\"\n",
        "# os.makedirs(output_folder, exist_ok=True)  # Create output directory if not exists\n",
        "\n",
        "# # Set target resolution\n",
        "# target_size = (300, 300)  # Change this as needed (e.g., 300x300 for B3)\n",
        "\n",
        "# # Get all image files\n",
        "# image_files = []\n",
        "# image_files.extend(glob(os.path.join(input_folder, \"**\", \"*.jpg\"), recursive=True))\n",
        "\n",
        "# # Process images\n",
        "# for img_path in image_files:\n",
        "#     # Determine relative path inside original folder\n",
        "#     relative_path = os.path.relpath(img_path, input_folder)\n",
        "\n",
        "#     # Create corresponding output path\n",
        "#     output_path = os.path.join(output_folder, relative_path)\n",
        "\n",
        "#     # Ensure the target directory exists\n",
        "#     os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "#     # Load and resize image\n",
        "#     img = cv2.imread(img_path)\n",
        "#     if img is None:\n",
        "#         print(f\"Skipping corrupted file: {img_path}\")\n",
        "#         continue\n",
        "\n",
        "#     img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "#     # Save the resized image\n",
        "#     cv2.imwrite(output_path, img_resized)\n",
        "\n",
        "# print(f\"Resized {len(image_files)} images and saved them to {output_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "dataset_path = \"upscaled/training\"\n",
        "batch_size = 32\n",
        "img_size = (300, 300)\n",
        "\n",
        "# Get class names from directory structure\n",
        "class_names = sorted(os.listdir(dataset_path))\n",
        "# class_names = ['Speed_20', 'Speed_30', 'Speed_50', 'Speed_60', 'Speed_70',\n",
        "#                'Speed_80','Speed_Limit_Ends', 'Speed_100', 'Speed_120', 'Overtaking_Prohibited',\n",
        "#                'Overtakeing_Prohibited_Trucks', 'Crossroad_Ahead', 'Priority_Road_Ahead', 'Yield', 'STOP',\n",
        "#                'Entry_Forbidden', 'Trucks_Forbidden', 'No_Entry(one-way traffic)', 'Cars_Prohibited(!)', 'Left_Curve_Ahead',\n",
        "#                'Right_Curve_Ahead', 'Bends_Left_Then_Right', 'Poor_Surface_Ahead', 'Slippery_Surface_Ahead', 'Road_Narrows_On_Right',\n",
        "#                'Roadwork_Ahead', 'Traffic_Light_Ahead', 'Warning_Pedestrians', 'Warning_Children', 'Warning_Bikes',\n",
        "#                'Uncontrolled_Crossroad', 'Deer_Crossing', 'End_Previous_Limitation', 'Turning_Right_Compulsory', 'Turning_Left_Compulsory',\n",
        "#                'Ahead_Only', 'Straight_Or_Right_Mandatory', 'Straight_Or_Left_Mandatory', 'Passing_Right_Compulsory', 'Passing_Left_Compulsory',\n",
        "#                'Roundabout', 'End_Overtaking_Prohibition', 'End_Overtaking_Prohibition_Trucks']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Get all image file paths\n",
        "image_paths = tf.io.gfile.glob(os.path.join(dataset_path, \"*\", \"*.jpg\"))  # Adjust extension if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path):\n",
        "    try:\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, img_size)\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        # Normalize to [-1, 1]\n",
        "        image = image / 255.0\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        tf.print(\"Error processing\", image_path, \":\", e)\n",
        "        # Return a tensor of zeros (or use tf.data.experimental.ignore_errors() downstream)\n",
        "        return tf.zeros([*img_size, 3], dtype=tf.float32)\n",
        "\n",
        "def get_label(image_path):\n",
        "    parts = tf.strings.split(image_path, os.sep)\n",
        "    label_str = parts[-2]  # Extract folder name (e.g., \"0\")\n",
        "    \n",
        "    try:\n",
        "        label = tf.strings.to_number(label_str, out_type=tf.int32)  # Convert safely to integer\n",
        "    except:\n",
        "        print(f\"Error converting label: {label_str}\")\n",
        "        label = tf.constant(-1, dtype=tf.int32)  # Assign -1 if an error occurs\n",
        "    \n",
        "    return label\n",
        "\n",
        "def adjust_brightness(image, min_brightness=0.4, max_brightness=1.0):\n",
        "    grayscale = tf.image.rgb_to_grayscale(image)\n",
        "    mean_brightness = tf.reduce_mean(grayscale)\n",
        "\n",
        "    brightness_factor = tf.maximum(min_brightness / mean_brightness, 1.0)  # Ensures valid scaling\n",
        "    adjusted_image = tf.clip_by_value(image * brightness_factor, 0.0, max_brightness)\n",
        "\n",
        "    return adjusted_image\n",
        "\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n",
        "\n",
        "# def augment(image_path, label):\n",
        "#     image = load_and_preprocess_image(image_path)  # Your function that loads & resizes images\n",
        "#     image = data_augmentation(image)  # Apply augmentation\n",
        "#     return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1741638291.661299   65669 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14000 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "data_augmentation = Sequential([\n",
        "    layers.Lambda(lambda x: adjust_brightness(x)),\n",
        "    layers.Rescaling(1./255),  # Normalize to [0,1]\n",
        "    layers.RandomFlip(\"horizontal\"),  # Random horizontal flips\n",
        "    layers.RandomRotation(0.2),  # Rotate by up to 10%\n",
        "    layers.RandomZoom(0.3),  # Random zoom\n",
        "    layers.RandomContrast(0.3),  # Adjust contrast slightly\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # print(get_label(image_paths[0]))  # Test the label extraction\n",
        "# for image in image_paths:\n",
        "#     img = cv2.imread(image)\n",
        "#     print(\"corrupted\" if img is None else \"\", sep='')  # If True, the image is corrupted\n",
        "\n",
        "# print(load_and_preprocess_image(image_paths[0]))  # Test the image loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "auto = tf.data.AUTOTUNE\n",
        "# Convert file paths into a tf.data.Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "# Apply preprocessing and labeling functions\n",
        "dataset = dataset.map(lambda x: (load_and_preprocess_image(x), get_label(x)), num_parallel_calls=auto)\n",
        "\n",
        "# Shuffle and split dataset\n",
        "dataset = dataset.shuffle(buffer_size=1024, seed=42)\n",
        "\n",
        "train_size = int(0.8 * len(image_paths))  # 80% for training\n",
        "train_ds = dataset.take(train_size).batch(batch_size).prefetch(auto)\n",
        "val_ds = dataset.skip(train_size).batch(batch_size).prefetch(auto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Compute class weights\n",
        "# class_labels = np.array([get_label(img_path).numpy() for img_path in image_paths])\n",
        "# class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(class_labels), y=class_labels)\n",
        "# class_weight_dict = dict(enumerate(class_weights))\n",
        "# pickle.dump(class_weight_dict, open(\"class_weight_dict.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weight_dict = pickle.load(open(\"class_weight_dict.pkl\", \"rb\"))\n",
        "# class_weight_dict = {int(k): float(v) for k, v in class_weight_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class <class 'int'>: <class 'numpy.float64'>\n"
          ]
        }
      ],
      "source": [
        "for k, v in class_weight_dict.items():\n",
        "    print(f\"Class {type(k)}: {type(v)}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(*img_size, 3))\n",
        "\n",
        "# Apply in-model data augmentation\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "''' EficientNetB0\n",
        "# Load EfficientNet and attach it\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=x)\n",
        "# base_model.trainable = False\n",
        "\n",
        "x = SpatialDropout2D(0.5)(base_model.output)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "'''\n",
        "\n",
        "''' Custom model'''\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Feature Extraction\n",
        "x = GlobalAveragePooling2D()(x)  # Replaces Flatten()\n",
        "\n",
        "# Fully Connected Layers\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "''''''\n",
        "\n",
        "\n",
        "# Define model explicitly\n",
        "model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                # metrics=['accuracy'])\n",
        "                metrics=[\n",
        "                    'accuracy',  \n",
        "                    tf.keras.metrics.Precision(name='precision'),\n",
        "                    tf.keras.metrics.Recall(name='recall')\n",
        "                    # F1Score(name='f1_score')\n",
        "                ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 999ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.02325764, 0.02325537, 0.0232521 , ..., 0.02325678, 0.02325448,\n",
              "        0.02325964],\n",
              "       [0.02325882, 0.02325521, 0.02324994, ..., 0.02325723, 0.02325425,\n",
              "        0.02326255],\n",
              "       [0.02325742, 0.02325494, 0.02325243, ..., 0.0232563 , 0.02325619,\n",
              "        0.02326008],\n",
              "       ...,\n",
              "       [0.02325719, 0.02325545, 0.02325302, ..., 0.02325636, 0.02325557,\n",
              "        0.02325924],\n",
              "       [0.02325695, 0.02325559, 0.02325359, ..., 0.0232564 , 0.023255  ,\n",
              "        0.02325826],\n",
              "       [0.02325742, 0.02325542, 0.02325278, ..., 0.02325635, 0.02325544,\n",
              "        0.02325953]], dtype=float32)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()  # Clear session to avoid clutter from old models\n",
        "warmup_data = tf.convert_to_tensor(next(iter(train_ds))[0])  # Get one batch\n",
        "model.predict(warmup_data)  # Run one inference step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: (32, 300, 300, 3)\n",
            "Label shape: (32,)\n",
            "Label sample: [14 14 30 30 14 14 14 30 14 14 14 14 30 30 30 14 30 14 14 14 14 14 14 30\n",
            " 30 30 30 14 14 30 14 14]\n"
          ]
        }
      ],
      "source": [
        "for image, label in train_ds.take(1):  # Take a sample batch\n",
        "    print(\"Image shape:\", image.shape)  # Should be (batch_size, height, width, channels)\n",
        "    print(\"Label shape:\", label.shape)  # Should be (batch_size,) for sparse labels\n",
        "    print(\"Label sample:\", label.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,  # Start small; increase if needed\n",
        "    validation_data=val_ds,\n",
        "    class_weight=class_weight_dict\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_filepath = \"models/custom_300_take1.keras\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(model_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Gen Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RnGBwGVZPyyh"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# View 9 images and their class labels\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mtrain_generator\u001b[49m)  \u001b[38;5;66;03m# Assuming train_generator is a generator\u001b[39;00m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m9\u001b[39m, batch_size)):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# View 9 images and their class labels\n",
        "plt.figure(figsize=(10, 10))\n",
        "images, labels = next(train_generator)  # Assuming train_generator is a generator\n",
        "batch_size = images.shape[0]\n",
        "\n",
        "for i in range(min(9, batch_size)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow((images[i] * 255).astype(\"uint8\"))\n",
        "    plt.title(int(labels[i]))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-Holdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to new images\n",
        "mini_path = \"original/mini_holdout\"  # Change this to your actual folder\n",
        "mini_files = sorted([os.path.join(mini_path, f) for f in os.listdir(mini_path) if f.endswith((\".jpg\"))])\n",
        "\n",
        "# Load and preprocess all new images\n",
        "mini = np.array([load_and_preprocess_image(img_path) for img_path in mini_files])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 100, 100, 3)\n",
            "(201, 100, 100, 3)\n"
          ]
        }
      ],
      "source": [
        "print(model.input_shape)\n",
        "print(mini.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
          ]
        }
      ],
      "source": [
        "# Predict class probabilities\n",
        "predictions = model.predict(mini)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "predicted_classes = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original/mini_holdout/00000.jpg 37\n",
            "original/mini_holdout/00010.jpg 37\n",
            "original/mini_holdout/00020.jpg 37\n",
            "original/mini_holdout/00030.jpg 37\n",
            "original/mini_holdout/00040.jpg 37\n",
            "original/mini_holdout/00050.jpg 37\n",
            "original/mini_holdout/00060.jpg 37\n",
            "original/mini_holdout/00070.jpg 37\n",
            "original/mini_holdout/00080.jpg 37\n",
            "original/mini_holdout/00090.jpg 37\n",
            "original/mini_holdout/00100.jpg 37\n",
            "original/mini_holdout/00110.jpg 37\n",
            "original/mini_holdout/00120.jpg 37\n",
            "original/mini_holdout/00130.jpg 37\n",
            "original/mini_holdout/00140.jpg 37\n",
            "original/mini_holdout/00150.jpg 37\n",
            "original/mini_holdout/00160.jpg 37\n",
            "original/mini_holdout/00170.jpg 37\n",
            "original/mini_holdout/00180.jpg 37\n",
            "original/mini_holdout/00190.jpg 37\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 200, 10):\n",
        "    print(mini_files[i], predicted_classes[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "mini_answers = pd.read_csv(\"mini_holdout_answers.csv\")\n",
        "# Create a mapping from filename → class\n",
        "filename_to_label = dict(zip(mini_answers[\"Filename\"], mini_answers[\"ClassId\"]))\n",
        "\n",
        "# Extract true labels in the same order as mini_files\n",
        "true_labels = [filename_to_label[os.path.basename(f)] for f in mini_files]\n",
        "\n",
        "# Convert to NumPy array (for compatibility with sklearn)\n",
        "true_labels = np.array(true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0000\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tman/tf-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/home/tman/tf-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_classes)\n",
        "precision = precision_score(true_labels, predicted_classes, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_classes, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_classes, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDc0xuoZs3DK"
      },
      "source": [
        "## Testing the model\n",
        "Once you have built and trained your model, the next step is to run the mini holdout images through it and see how well your model does at making predictions for images it has never seen before.\n",
        "\n",
        "Since loading these images and formatting them for the model can be tricky, you may find the following code useful. This code only uses your model to predict the class label for a given image. You'll still need to compare those predictions to the \"ground truth\" class labels in `mini_holdout_answers.csv` to evaluate how well the model does.\n",
        "\n",
        "Previously, you were given a file that would check your results. This time you're given the answers to the first mini holdout dataset. You'll need to compare those predictions against the \"ground truth\" class labels in `mini_holdout_answers.csv` to evaluate how well the model does.\n",
        "\n",
        "Make sure to use the insights gained from the mini hold out dataset in your executive summary.\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "test_dir = '/content/'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        classes=['mini_holdout'],\n",
        "        target_size=image_size,\n",
        "        class_mode='sparse',\n",
        "        shuffle=False)\n",
        "probabilities = model.predict(test_generator)\n",
        "predictions = [np.argmax(probas) for probas in probabilities]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dodPABZ-Yz-6"
      },
      "source": [
        "##Mini Hold out Dataset\n",
        "\n",
        "\n",
        "Once you feel confident, you will need to predict for the full holdout dataset using the following code, and submit your csv file:\n",
        "\n",
        "```\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "test_dir = '/content/'\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        classes=['holdout'],\n",
        "        target_size=image_size,\n",
        "        class_mode='sparse',\n",
        "        shuffle=False)\n",
        "probabilities = model.predict(test_generator)\n",
        "predictions = [np.argmax(probas) for probas in probabilities]\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_signs_v2_student.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.12 (tf-env)",
      "language": "python",
      "name": "tf-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
