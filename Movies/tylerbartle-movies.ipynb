{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import numpy as np\n",
    "# import os\n",
    "import pandas as pd\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_CATEGORY_DFS = False\n",
    "KEEP_SETS = False\n",
    "DROPS = {'movie_name', 'description', 'director', 'star'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Combining the CSVs as below is not used anymore, as the data is dealt with in DataFrames before concatenation.\n",
    "\"\"\"\n",
    "# Get all CSV file paths in the folder\n",
    "csv_files = glob.glob(os.path.join(\"Data\", '*.csv'))\n",
    "\n",
    "# Combine them\n",
    "combined_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# Save the result\n",
    "combined_df.to_csv('movies.csv', index=False)\n",
    "\n",
    "print(f\"Combined {len(csv_files)} files into 'movies.csv'.\")\n",
    "\"\"\"\n",
    "\n",
    "categories = {\n",
    "    'action',\n",
    "    'adventure',\n",
    "    'animation',\n",
    "    'biography',\n",
    "    'crime',\n",
    "    'family',\n",
    "    'fantasy',\n",
    "    'film_noir',\n",
    "    'history',\n",
    "    'horror',\n",
    "    'mystery',\n",
    "    'romance',\n",
    "    'scifi',\n",
    "    'sports',\n",
    "    'thriller',\n",
    "    'war'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Rather than defining each category DataFrame manually, the loop below defines them dynamically.\n",
    "\"\"\"\n",
    "action = pd.read_csv('Data/action.csv')\n",
    "adventure = pd.read_csv('Data/adventure.csv')\n",
    "animation = pd.read_csv('Data/animation.csv')\n",
    "biography = pd.read_csv('Data/biography.csv')\n",
    "crime = pd.read_csv('Data/crime.csv')\n",
    "family = pd.read_csv('Data/family.csv')\n",
    "fantasy = pd.read_csv('Data/fantasy.csv')\n",
    "film_noir = pd.read_csv('Data/film_noir.csv')\n",
    "history = pd.read_csv('Data/history.csv')\n",
    "horror = pd.read_csv('Data/horror.csv')\n",
    "mystery = pd.read_csv('Data/mystery.csv')\n",
    "romance = pd.read_csv('Data/romance.csv')\n",
    "scifi = pd.read_csv('Data/scifi.csv')\n",
    "sports = pd.read_csv('Data/sports.csv')\n",
    "thriller = pd.read_csv('Data/thriller.csv')\n",
    "war = pd.read_csv('Data/war.csv')\n",
    "\"\"\"\n",
    "\n",
    "for category in categories:\n",
    "    # Dynamically read each CSV file into a pd.DataFrame based on the category name\n",
    "        # and assign it to a global variable with the same name as the category\n",
    "    globals()[category] = pd.read_csv(f\"Data/{category}.csv\")\n",
    "\n",
    "del category\n",
    "\n",
    "# globals()['action'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_split_columns(df: pd.DataFrame, splits: Iterable, make_sets: bool) -> None:\n",
    "    \"\"\"\n",
    "    Cleans and splits specified columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df: The DataFrame to process.\n",
    "        splits: List of columns to split and clean.\n",
    "        make_sets: If `True`, creates a set of unique values for each column.\n",
    "    \n",
    "    Returns:\n",
    "        None: The function modifies the DataFrame in place.\n",
    "    \"\"\"\n",
    "\n",
    "    def split_multiples(df: pd.DataFrame, col: str) -> None:\n",
    "        \"\"\"\n",
    "        Splits the values in a column by `,` and stores them in a new column, as a list.\n",
    "\n",
    "        Parameters:\n",
    "            df: The DataFrame to process.\n",
    "            col: The column to split.\n",
    "        \n",
    "        Returns:\n",
    "            None: The function modifies the DataFrame in place.\n",
    "        \"\"\"\n",
    "\n",
    "        df[col + 's'] = \\\n",
    "            df[col].apply(\n",
    "                lambda x: {val.strip().lower() for val in x.split(',')}\n",
    "                    if pd.notnull(x) \n",
    "                    else None)\n",
    "        \n",
    "    def trim_name_urls(df: pd.DataFrame, col: str) -> None:\n",
    "        \"\"\"\n",
    "        Trims name URLs to just the important part.\n",
    "        This is used for director and star names.\n",
    "        \n",
    "        i.e. `'/name/nm0000001/'` becomes `'nm0000001'`\n",
    "        \n",
    "        Parameters:\n",
    "            df: The DataFrame to process.\n",
    "            col: The column to trim.\n",
    "        \n",
    "        Returns:\n",
    "            None: The function modifies the DataFrame in place.\n",
    "        \"\"\"\n",
    "\n",
    "        df[col] = df[col].apply(\n",
    "            lambda s: {x.split('/')[-2].strip() for x in s}\n",
    "                if isinstance(s, set) and len(s) > 0\n",
    "                else s)\n",
    "\n",
    "    if make_sets:\n",
    "        def track_uniques(df: pd.DataFrame, col: str, unique_set: set) -> None:\n",
    "            \n",
    "            \"\"\"\n",
    "            Tracks unique values in a column and stores them in a set.\n",
    "\n",
    "            Parameters:\n",
    "                df: The DataFrame to process.\n",
    "                col: The column to track unique values from.\n",
    "                unique_set: A set to store unique values.\n",
    "            \n",
    "            Returns:\n",
    "                None: The function modifies the set in place.\n",
    "            \"\"\"\n",
    "\n",
    "            for items in df[col + 's'].dropna():\n",
    "                # Add each item to the set\n",
    "                for item in items:\n",
    "                    unique_set.add(item)\n",
    "\n",
    "    ##########\n",
    "\n",
    "    for col in splits:\n",
    "        col_s = col + 's'\n",
    "        try:\n",
    "            # Split the column values by ',' and store in a new column\n",
    "            split_multiples(df, col)\n",
    "\n",
    "            # Trim name URLs for director and star names if applicable\n",
    "            if col_s in ['director_ids', 'star_ids']:\n",
    "                trim_name_urls(df, col_s)\n",
    "            elif col_s == 'genres':\n",
    "                # Convert genres to lowercase\n",
    "                g = {df['category'][0]}\n",
    "                df[col_s] = df[col_s].apply(lambda s: s | g)\n",
    "                df.drop(columns=['category'], inplace=True)\n",
    "\n",
    "            if make_sets:\n",
    "                # Track unique values for each column\n",
    "                globals()[col_s] = set()\n",
    "                track_uniques(df, col, globals()[col_s])\n",
    "\n",
    "            df.drop(columns=[col], inplace=True) # Drop the original column to save space\n",
    "        except KeyError:\n",
    "            # Handle the case where the column might not exist in the DataFrame (i.e. this function has already been performed)\n",
    "            print(f\"Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "            if make_sets:\n",
    "                # Initialize an empty set to avoid errors later on\n",
    "                if col_s not in globals():\n",
    "                    globals()[col_s] = set()\n",
    "\n",
    "def clean_runtime(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Cleans the runtime column in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df: The DataFrame to process.\n",
    "    \n",
    "    Returns:\n",
    "        None: The function modifies the DataFrame in place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert runtime to numeric values (in minutes)\n",
    "    df['runtime'] = df['runtime'].apply(\n",
    "        lambda x: int(x.split()[0].replace(',', '')) if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(category: str, drops: Iterable = None, make_sets: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Performs all clean-up / preprocessing steps on the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df: The DataFrame to process.\n",
    "        drops: List of columns to drop from the DataFrame.\n",
    "        make_sets: If `True`, creates a set of unique values for each column.\n",
    "    \n",
    "    Returns:\n",
    "        None: The function modifies the DataFrame in place.\n",
    "    \"\"\"\n",
    "    df = globals()[category]\n",
    "\n",
    "    df['category'] = category\n",
    "    \n",
    "    df.dropna(inplace=True, how='all') # Drop rows where all elements are None\n",
    "    try:\n",
    "        df.drop(columns=drops, inplace=True) # Drop specified columns\n",
    "    except:\n",
    "        # Handle the case where the columns might not exist in the DataFrame\n",
    "        print(f\"Columns {drops} not found in DataFrame.\\nHas preprocessing already been performed?\")\n",
    "        return\n",
    "\n",
    "    # NOTE: splits is a list of columns to split by ','.\n",
    "        # 'director' and 'star' columns are likely not useful,\n",
    "        # as they are just text (meaningless to a model, likely),\n",
    "        # so if they are included in drops, they will be ignored here.\n",
    "    splits = {\n",
    "        'genre',\n",
    "        'director',\n",
    "        'director_id',\n",
    "        'star',\n",
    "        'star_id',\n",
    "    } - drops\n",
    "\n",
    "    clean_and_split_columns(df, splits, make_sets)\n",
    "    clean_runtime(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    # Prepare each category DataFrame\n",
    "    prepare(category, drops=DROPS, make_sets=KEEP_SETS)\n",
    "\n",
    "del category\n",
    "\n",
    "# NOTE: now, each category's DataFrame has:\n",
    "# - Empty rows removed,\n",
    "# - useless columns dropped,\n",
    "# - list-like columns split into lists and the original columns removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: testing\n",
    "\n",
    "# type(globals()['action']['genres'][0])\n",
    "# print(globals()['genres'])\n",
    "# globals()['action']['certificate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all DataFrames in categories into one DataFrame\n",
    "combined_df = pd.concat([globals()[category] for category in categories], ignore_index=True)\n",
    "# combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Exploring duplicate movie IDs\n",
    "\n",
    "# # Find movie_ids that are duplicated across all DataFrames\n",
    "# duplicated_ids = combined_df['movie_id'][combined_df['movie_id'].duplicated(keep=False)]\n",
    "\n",
    "# # Filter rows where movie_id is in the list of duplicated IDs\n",
    "# duplicates = combined_df[combined_df['movie_id'].isin(duplicated_ids)]\n",
    "# # duplicates.head()\n",
    "# duplicates[duplicates['movie_name'] == 'Black Panther: Wakanda Forever'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Minimizing bloat for testing aggregation\n",
    "# comb_example = combined_df[['movie_id', 'movie_name', 'genres']]\n",
    "# # comb_example.head()\n",
    "# dupl_example = duplicates[['movie_id', 'movie_name', 'genres']]\n",
    "# # dupl_example[dupl_example['movie_name'] == 'Black Panther: Wakanda Forever'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all rows with the same movie_id into a single row:\n",
    "# union all genre sets, and keep the first-appearing value for other columns.\n",
    "\n",
    "# Identify the columns other than movie_id and genre\n",
    "other_cols = [col for col in combined_df.columns if col not in ['movie_id', 'genres']]\n",
    "\n",
    "# Create an aggregation dictionary:\n",
    "#   - for all \"other\" columns, take 'first'\n",
    "#   - for 'genre', union the sets\n",
    "agg_dict = {col: 'first' for col in other_cols}\n",
    "agg_dict['genres'] = lambda genre_series: set().union(*genre_series)\n",
    "\n",
    "# Perform the groupby aggregation\n",
    "grouped_df = combined_df.groupby('movie_id', as_index=False).agg(agg_dict)\n",
    "\n",
    "# grouped_df.info()\n",
    "# grouped_df[grouped_df['movie_name'] == 'Black Panther: Wakanda Forever'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.info()\n",
    "# grouped_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is how many duplicate movie_ids were removed:\n",
    "# print(combined_df.shape[0] - grouped_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = grouped_df.copy()\n",
    "\n",
    "to_del = {'combined_df', 'duplicated_ids', 'duplicates', 'comb_example', 'dupl_example', 'other_cols', 'agg_dict', 'grouped_df'}\n",
    "if not KEEP_CATEGORY_DFS:\n",
    "    to_del |= categories\n",
    "\n",
    "for item in to_del:\n",
    "    try:\n",
    "        del globals()[item]\n",
    "    except KeyError:\n",
    "        pass\n",
    "del to_del\n",
    "del item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(id: str, type: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a link to the movie or person based on the ID and type.\n",
    "\n",
    "    Parameters:\n",
    "        id: The ID of the movie or person.\n",
    "        type: The type of the link (`'movie'` or `'person'`).\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted link.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.imdb.com\"\n",
    "    if   type == 'movie':\n",
    "        return f\"{base_url}/title/{id}/\"\n",
    "    elif type == 'person':\n",
    "        return f\"{base_url}/name/{id}/\"\n",
    "    else:\n",
    "        raise ValueError(\"Type must be either 'movie' or 'person'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.imdb.com/title/tt0000009/\n"
     ]
    }
   ],
   "source": [
    "print(get_link(movies['movie_id'][0], 'movie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies['genres'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
